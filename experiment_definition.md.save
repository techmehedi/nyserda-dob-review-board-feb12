# Experiment Definition: DOB Job Type Classification (Pilot)

## Task
The task is **Job Type Classification** for DOB permit descriptions.

Given a short free-text job description, the model must predict **exactly one job type category** from the official DOB taxonomy (e.g., Mechanical Systems, Plumbing, General Construction, Sidewalk Shed, etc.).

This experiment does **not** perform electrification or decarbonization (YES/NO) classification. Any references to electrification in earlier drafts were placeholders and are no longer applicable.

---

## Dataset
We use a sampled subset of the DOB approved permits dataset.

Each row contains:
- `input`: Job description text
- `ground_truth`: Official DOB job type label

The dataset used for this pilot contains **5,000 rows**.

---

## Systems Compared

### 1) Baseline: Single-Model Classification
- Model: `meta-llama/Llama-3.2-3B-Instruct`
- Setup:
  - The model receives the job description
  - It independently predicts a single job type label
- Output:
  - Parsed and normalized to the `ground_truth` label set

This corresponds to **Round 1 (R1) Llama** outputs.

---

### 2) Review Board: Multi-Agent with Arbiter
The review board follows a **multi-agent deliberation + arbiter** design.

#### Round 1: Independent Agent Proposals
Three models independently classify the same job description:
- `meta-llama/Llama-3.2-3B-Instruct`
- `Qwen/Qwen3-4B-Instruct-2507`
- `google/gemma-3-4b-it`

Each agent proposes a job type label along with reasoning.

#### Shared Deliberation Context
All Round 1 outputs are aggregated into a shared discussion context (`history_raw`), representing cross-agent perspectives.

#### Round 2: Arbiter Decision
- Model: `meta-llama/Llama-3.2-3B-Instruct`
- Input:
  - Original job description
  - Full multi-agent discussion from Round 1
- Output:
  - Final job type classification

The Round 2 Llama output is treated as the **final review board decision**.

Note: While three agents participate in Round 1, the final decision is produced by a single arbiter model in Round 2.

---

## Label Normalization
Raw model outputs are post-processed to extract and normalize predicted labels to the official DOB job type taxonomy.

Predictions that cannot be mapped to a valid job type are marked as **off-schema**.

---

## Evaluation Metrics
Evaluation is performed only on rows where both:
- `ground_truth` is present
- A valid predicted label is extracted

Metrics reported:
- Accuracy
- Macro-averaged F1
- Weighted F1
- Number of evaluated rows (`n_eval`)

---

## Off-Schema (Hallucination Proxy)
For this classification task, “hallucination” is defined as an **off-schema output**:
- The model produces an answer that cannot be mapped to any valid DOB job type label

We report:
- Baseline off-schema rate
- Review board off-schema rate

This is a structural proxy and does **not** represent factual hallucination.

---

## Outputs
The experiment produces the following artifacts:
- `dob_review_board_results.csv`
  - Cleaned predictions, normalized labels, and flags
- `dob_tiny_results_table.csv`
  - Summary metrics for baseline vs review board
- `dob_review_board_analysis.ipynb`
  - Full analysis, metrics, and visualizations

These artifacts are used for reporting and for the Hugging Face interactive demo.
